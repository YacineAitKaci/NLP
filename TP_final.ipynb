{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Lu\n",
      "Corpus après prétraitement :\n",
      " ['im feeling rather rotten im ambitious right', 'im updating blog feel shitty', 'never make separate ever want feel like ashamed', 'left bouquet red yellow tulip arm feeling slightly optimistic arrived', 'feeling little vain one']\n"
     ]
    }
   ],
   "source": [
    "#LIRE LE DATASET\n",
    "\n",
    "test = pd.read_csv('Dataset/test.csv')\n",
    "print(\"Test Dataset Lu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PRETRAITEMENT DU DATASET\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def nettoyer(text):\n",
    "    text = text.lower()\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenisation\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Suppression des mots vides et lemmatisation\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    # Reconstituer le texte prétraité\n",
    "    return ' '.join(words)\n",
    "test['processed_text'] = test['text'].apply(nettoyer)\n",
    "# Convertir en format de corpus TF-IDF (liste de documents)\n",
    "corpus = test['processed_text'].tolist()\n",
    "# Vérification\n",
    "print(\"Corpus après prétraitement :\\n\", corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
